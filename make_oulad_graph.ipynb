{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roh/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def one_hot_encoding(n, n_to_index):\n",
    "    one_hot_vector = [0]*(len(n_to_index))\n",
    "    index = n_to_index[n]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "# learning_sequence에 이미 들어 있는 activity를 제외하고 다음 activity를 추가함\n",
    "def add_learning_sequence(num_next_learning, learning_sequence):\n",
    "    for i in range(len(num_next_learning)):\n",
    "        if num_next_learning[i][0] in learning_sequence:\n",
    "            pass\n",
    "        else:\n",
    "            learning_sequence.append(num_next_learning[i][0])\n",
    "            break\n",
    "    return learning_sequence, num_next_learning[i][0]\n",
    "\n",
    "# list에 있는 activity들의 개수를 count함\n",
    "def count_activity(first_learning):\n",
    "    num_activity = {}\n",
    "    for activity in list(set(first_learning)):\n",
    "        num_activity[activity] = first_learning.count(activity)\n",
    "    num_activity = sorted(num_activity.items(),reverse=True, key=lambda x:x[1])\n",
    "    return num_activity\n",
    "\n",
    "# next_learning이라는 리스트에 우리의 target_activity 다음에 오는 activity들을 추가함\n",
    "def find_next_learning(student_list, target_activity, student_learning):\n",
    "    next_learning = []\n",
    "    for student in student_list:\n",
    "        first_activity_index = [i for i in range(len(student_learning[student])) if target_activity == student_learning[student][i]]\n",
    "        for index in first_activity_index:\n",
    "            if index+1 >= len(student_learning[student]):\n",
    "                pass\n",
    "            else:\n",
    "                next_learning.append(student_learning[student][index+1])\n",
    "    return next_learning\n",
    "\n",
    "# MultiIndex 컬럼을 평탄화 하는 함수\n",
    "def flat_cols(df):\n",
    "    df.columns = [' / '.join(x) for x in df.columns.to_flat_index()]\n",
    "    return df\n",
    "\n",
    "courses = pd.read_csv('data/archive/courses.csv')\n",
    "vle = pd.read_csv('data/archive/vle.csv')\n",
    "studentVle = pd.read_csv('data/archive/studentVle.csv')\n",
    "studentRegistration = pd.read_csv('data/archive/studentRegistration.csv')\n",
    "studentAssessment = pd.read_csv('data/archive/studentAssessment.csv')\n",
    "studentInfo = pd.read_csv('data/archive/studentInfo.csv')\n",
    "assessments = pd.read_csv('data/archive/assessments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:18<00:00,  1.18it/s]\n",
      "/tmp/ipykernel_42492/2696520996.py:53: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  group_studentvle = studentVle.groupby(['id_student', 'id_site']).sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Completion == 1, Dropout == 0\n",
    "studentInfo['completion_status'] = list(map(lambda x: 1 if (x == 'Pass') or (x == 'Distinction') else 0, studentInfo['final_result']))\n",
    "studentInfo['course_name'] = studentInfo['code_module'] + '_' + studentInfo['code_presentation']\n",
    "vle['course_name'] = vle['code_module'] + '_' + vle['code_presentation']\n",
    "studentVle['course_name'] = studentVle['code_module'] + '_' + studentVle['code_presentation']\n",
    "studentVle['graph_name'] = studentVle['code_module'] + '_' + studentVle['code_presentation'] + '_' + studentVle['id_student'].astype(str)\n",
    "\n",
    "# 첫번째 인터랙션 날을 기준으로 이후 30일간으로 맞추기 위해 첫번째 인터랙션 날을 기준으로 뺄셈\n",
    "student_first_date = studentVle[['graph_name', 'date']].groupby('graph_name').agg(['min']).pipe(flat_cols)\n",
    "studentVle = pd.merge(studentVle, student_first_date, how='left', on='graph_name')\n",
    "studentVle['new_date'] = studentVle['date'] - studentVle['date / min']\n",
    "\n",
    "course_learning_sequence = {}\n",
    "learning_sequence = []\n",
    "\n",
    "studentVle = studentVle[studentVle['new_date'] < 30]\n",
    "\n",
    "for course in tqdm(list(set(vle['course_name']))):\n",
    "    target_course = studentVle[studentVle['course_name'] == course]\n",
    "\n",
    "    target_course_student = set(target_course['id_student'].to_list())\n",
    "    target_course_activity = set(target_course['id_site'].to_list())\n",
    "\n",
    "    student_learning = {}\n",
    "    first_learning = []\n",
    "\n",
    "    # 학생들 별로 learning sequence 담기\n",
    "    for student in target_course_student:\n",
    "        learing_list = target_course[target_course['id_student'] == student]['id_site'].to_list()\n",
    "        student_learning[student] = learing_list\n",
    "        first_learning.append(learing_list[0])\n",
    "\n",
    "    # 첫번째 시작 activity 계산\n",
    "    num_activity = count_activity(first_learning)\n",
    "    target_activity = num_activity[0][0]\n",
    "    learning_sequence.append(target_activity)\n",
    "\n",
    "    for i in range(len(target_course_activity)):\n",
    "        bf_ls_len = len(learning_sequence)\n",
    "        next_learning = find_next_learning(target_course_student, target_activity, student_learning)\n",
    "        num_next_learning = count_activity(next_learning)\n",
    "        learning_sequence, target_activity = add_learning_sequence(num_next_learning, learning_sequence)\n",
    "        if bf_ls_len == len(learning_sequence):\n",
    "            break\n",
    "    \n",
    "    course_learning_sequence[course] = learning_sequence\n",
    "    \n",
    "# Graph build 순서\n",
    "# course node 만들기 -> activity node 만들기 -> activity node와 course node 사이에 edge 생성\n",
    "# activity node 사이에 edge만들기\n",
    "# student node만들기 -> activity node와 edge 생성 -> course node와 edge 생성\n",
    "studentVle = studentVle[studentVle['date'] >= 0]\n",
    "group_studentvle = studentVle.groupby(['id_student', 'id_site']).sum().reset_index()\n",
    "\n",
    "node_set = {}\n",
    "index = 0\n",
    "course_node = list(set(vle['course_name']))\n",
    "activity_node = list(set(vle['id_site']))\n",
    "student_node = list(set(studentInfo['id_student']))\n",
    "for i in course_node + activity_node + student_node:\n",
    "    node_set[i] = index\n",
    "    index += 1\n",
    "    \n",
    "edge_feature = []\n",
    "# course node, activity node 만들기 -> activity node와 course node 사이에 edge 생성 가중치 1\n",
    "src_node, dst_node = [], []\n",
    "for i in range(len(vle)):\n",
    "    #src_node.append(node_set[vle['course_name'].to_numpy()[i]])\n",
    "    #dst_node.append(node_set[vle['id_site'].to_numpy()[i]])\n",
    "    src_node.append(node_set[vle['course_name'][i]])\n",
    "    dst_node.append(node_set[vle['id_site'][i]])\n",
    "    edge_feature.append(1)\n",
    "    \n",
    "    \n",
    "# student node만들기 -> activity node와 edge 생성 click_sum z-score\n",
    "for i in range(len(group_studentvle)):\n",
    "    src_node.append(node_set[group_studentvle['id_student'][i]])\n",
    "    dst_node.append(node_set[group_studentvle['id_site'][i]])\n",
    "    edge_feature.append(group_studentvle['sum_click'][i])\n",
    "    #edge_feature.append(group_studentvle[(group_studentvle['id_student'] == studentVle['id_student'][i]) & (group_studentvle['id_site'] == studentVle['id_site'][i])]['sum_click'].values[0])\n",
    "    \n",
    "# course node와 edge 생성 completion한 사람만 edge 생성 가중치 1\n",
    "for i in range(len(studentInfo)):\n",
    "    if studentInfo['completion_status'][i] == 'Completion':\n",
    "        src_node.append(node_set[studentInfo['id_student'][i]])\n",
    "        dst_node.append(node_set[studentInfo['course_name'][i]])\n",
    "        edge_feature.append(1)\n",
    "        \n",
    "# activity 사이에 edge 생성\n",
    "for course in list(set(vle['course_name'])):\n",
    "    for i in range(len(course_learning_sequence[course])):\n",
    "        src_node.append(node_set[course])\n",
    "        dst_node.append(node_set[course_learning_sequence[course][i]])\n",
    "        edge_feature.append(1)\n",
    "\n",
    "graph_src_node = src_node + dst_node\n",
    "graph_dst_node = dst_node + src_node\n",
    "g = dgl.graph((graph_src_node, graph_dst_node))\n",
    "\n",
    "g.edata['edge_feature'] = torch.FloatTensor(edge_feature + edge_feature)\n",
    "\n",
    "node_feature = []\n",
    "\n",
    "# make one-hot vector using activity_type\n",
    "activity_onehot_list = list(vle['activity_type'].unique())\n",
    "activity_zero_list = [0 for i in range(len(activity_onehot_list))]\n",
    "activity_to_index = {n : index for index, n in enumerate(activity_onehot_list)}\n",
    "activity_type = []\n",
    "for activity_id in activity_node:\n",
    "    activity_type.append(vle[vle['id_site'] == activity_id]['activity_type'].values[0])\n",
    "    \n",
    "# activity one-hot\n",
    "activity_node_feature = []\n",
    "for activity_type in activity_type:\n",
    "    activity_node_feature.append(one_hot_encoding(activity_type, activity_to_index))\n",
    "    \n",
    "# date one-hot\n",
    "date_feature = []\n",
    "for student in student_node:\n",
    "    base_date_feature = [0]*30\n",
    "    student_date = list(set(studentVle[studentVle['id_student'] == student]['new_date']))\n",
    "    for date in student_date:\n",
    "        base_date_feature[date] = 1\n",
    "    date_feature.append(base_date_feature)\n",
    "base_date_feature = [0]*30\n",
    "\n",
    "for i in range(len(course_node)):\n",
    "    node_feature.append([0,0,0] + activity_zero_list + base_date_feature)\n",
    "\n",
    "for i in range(len(activity_node)):\n",
    "    node_feature.append([1,0,0] + activity_node_feature[i] + base_date_feature)\n",
    "    \n",
    "for i in range(len(student_node)):\n",
    "    node_feature.append([0,1,0] + activity_zero_list + date_feature[i])\n",
    "    \n",
    "g.ndata['feature'] = torch.FloatTensor(node_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "course_student = {}\n",
    "pos_student = studentInfo[studentInfo['completion_status'] == 1] # Completion 15385\n",
    "neg_student = studentInfo[studentInfo['completion_status'] == 0] # Dropout 17208\n",
    "train_pos, train_neg, test_pos, test_neg = {'src':[], 'dst':[]}, {'src':[], 'dst':[]}, {'src':[], 'dst':[]}, {'src':[], 'dst':[]}\n",
    "for course in course_node:\n",
    "    pos_src_list = []\n",
    "    pos_dst_list = []\n",
    "    \n",
    "    neg_src_list = []\n",
    "    neg_dst_list = []\n",
    "    \n",
    "    for i in pos_student[pos_student['course_name'] == course]['id_student'].tolist():\n",
    "        pos_src_list.append(node_set[i])\n",
    "        pos_dst_list.append(node_set[course])\n",
    "        \n",
    "    for i in neg_student[neg_student['course_name'] == course]['id_student'].tolist():\n",
    "        neg_src_list.append(node_set[i])\n",
    "        neg_dst_list.append(node_set[course])\n",
    "    \n",
    "    train_pos_src, test_pos_src, train_pos_dst, test_pos_dst = train_test_split(pos_src_list, pos_dst_list, test_size=0.2, train_size=0.8, random_state=32)\n",
    "    train_neg_src, test_neg_src, train_neg_dst, test_neg_dst = train_test_split(neg_src_list, neg_dst_list, test_size=0.2, train_size=0.8, random_state=32)\n",
    "\n",
    "    train_pos['src'] = train_pos['src'] + train_pos_src\n",
    "    train_pos['dst'] = train_pos['dst'] + train_pos_dst\n",
    "    train_neg['src'] = train_neg['src'] + train_neg_src\n",
    "    train_neg['dst'] = train_neg['dst'] + train_neg_dst\n",
    "    test_pos['src'] = test_pos['src'] + test_pos_src\n",
    "    test_pos['dst'] = test_pos['dst'] + test_pos_dst\n",
    "    test_neg['src'] = test_neg['src'] + test_neg_src\n",
    "    test_neg['dst'] = test_neg['dst'] + test_neg_dst\n",
    "    \n",
    "num_node = len(course_node + activity_node + student_node)\n",
    "train_pos_g = dgl.graph((train_pos['src'], train_pos['dst']), num_nodes=num_node)\n",
    "train_neg_g = dgl.graph((train_neg['src'], train_neg['dst']), num_nodes=num_node)\n",
    "test_pos_g = dgl.graph((test_pos['src'], test_pos['dst']), num_nodes=num_node)\n",
    "test_neg_g = dgl.graph((test_neg['src'], test_neg['dst']), num_nodes=num_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "from dgl.nn import SAGEConv, GraphConv\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dgl import save_graphs, load_graphs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_layers, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convlayers = nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                self.convlayers.append(\n",
    "                    GraphConv(in_feats, h_feats)\n",
    "                )\n",
    "            else:\n",
    "                self.convlayers.append(\n",
    "                    GraphConv(h_feats, h_feats)\n",
    "                )\n",
    "            # Initialize the weights with xavier_uniform\n",
    "            nn.init.xavier_uniform_(self.convlayers[-1].weight)\n",
    "        #self.conv_out = GraphConv(h_feats, num_classes)\n",
    "        \n",
    "        #self.mlp = MLP(h_feats, 8, num_classes)\n",
    "        #nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        \n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['feature']\n",
    "        e = g.edata['edge_feature']\n",
    "    \n",
    "        for i, layer in enumerate(self.convlayers):\n",
    "            h = layer(g, h, edge_weight=e)\n",
    "            h = F.relu(h)\n",
    "            \n",
    "        # h = self.conv_out(g, h, edge_weight=e)\n",
    "        # g.ndata[\"h\"] = h\n",
    "        # return dgl.mean_nodes(g, \"h\")\n",
    "    \n",
    "        \n",
    "        #last_node = g.num_nodes() - 1  # index of last node\n",
    "        #date_node = g.ndata['node_type'].tolist().count([0,0,1])\n",
    "        #h = self.mlp(h[last_node])\n",
    "        #return h\n",
    "    \n",
    "        # h = self.conv_out(g, h, edge_weight=e)\n",
    "        # date_node = g.ndata['node_type'].tolist().count([0,0,1])\n",
    "        # return h[date_node]\n",
    "        \n",
    "        #date_node = g.ndata['node_type'].tolist().count([0,0,1])\n",
    "        #h = self.mlp(h[-date_node:].sum(dim=0)/date_node)\n",
    "        return h\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, 1)\n",
    "        \n",
    "    def apply_edges(self, edges):\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']\n",
    "        \n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 3.273742437362671\n",
      "In epoch 5, loss: 1.6731860637664795\n",
      "In epoch 10, loss: 0.8603308200836182\n",
      "In epoch 15, loss: 0.8703266978263855\n",
      "In epoch 20, loss: 0.7849550247192383\n",
      "In epoch 25, loss: 0.688713788986206\n",
      "In epoch 30, loss: 0.6854637265205383\n",
      "In epoch 35, loss: 0.656919002532959\n",
      "In epoch 40, loss: 0.6564443707466125\n",
      "In epoch 45, loss: 0.6490513682365417\n",
      "In epoch 50, loss: 0.6424835920333862\n",
      "In epoch 55, loss: 0.6366294622421265\n",
      "In epoch 60, loss: 0.6315904855728149\n",
      "In epoch 65, loss: 0.6281233429908752\n",
      "In epoch 70, loss: 0.6254758834838867\n",
      "In epoch 75, loss: 0.6237497329711914\n",
      "In epoch 80, loss: 0.6223864555358887\n",
      "In epoch 85, loss: 0.6209548115730286\n",
      "In epoch 90, loss: 0.6199554800987244\n",
      "In epoch 95, loss: 0.61895352602005\n",
      "In epoch 100, loss: 0.6179776191711426\n",
      "In epoch 105, loss: 0.6171786785125732\n",
      "In epoch 110, loss: 0.6164430379867554\n",
      "In epoch 115, loss: 0.6157175302505493\n",
      "In epoch 120, loss: 0.6150180697441101\n",
      "In epoch 125, loss: 0.6143000721931458\n",
      "In epoch 130, loss: 0.6136400103569031\n",
      "In epoch 135, loss: 0.6130130887031555\n",
      "In epoch 140, loss: 0.6124395728111267\n",
      "In epoch 145, loss: 0.6119061708450317\n",
      "In epoch 150, loss: 0.6113912463188171\n",
      "In epoch 155, loss: 0.6109127402305603\n",
      "In epoch 160, loss: 0.6104629039764404\n",
      "In epoch 165, loss: 0.6100291013717651\n",
      "In epoch 170, loss: 0.6095851063728333\n",
      "In epoch 175, loss: 0.6091073751449585\n",
      "In epoch 180, loss: 0.6086521148681641\n",
      "In epoch 185, loss: 0.6081741452217102\n",
      "In epoch 190, loss: 0.6076787710189819\n",
      "In epoch 195, loss: 0.6071934103965759\n",
      "In epoch 200, loss: 0.6067184805870056\n",
      "In epoch 205, loss: 0.6062180995941162\n",
      "In epoch 210, loss: 0.6057876944541931\n",
      "In epoch 215, loss: 0.6053251624107361\n",
      "In epoch 220, loss: 0.6049008369445801\n",
      "In epoch 225, loss: 0.6044635772705078\n",
      "In epoch 230, loss: 0.6040520668029785\n",
      "In epoch 235, loss: 0.6036412119865417\n",
      "In epoch 240, loss: 0.6032439470291138\n",
      "In epoch 245, loss: 0.6028716564178467\n",
      "In epoch 250, loss: 0.6025381088256836\n",
      "In epoch 255, loss: 0.6022517085075378\n",
      "In epoch 260, loss: 0.6019371151924133\n",
      "In epoch 265, loss: 0.601586103439331\n",
      "In epoch 270, loss: 0.6013312339782715\n",
      "In epoch 275, loss: 0.6009876728057861\n",
      "In epoch 280, loss: 0.6006506681442261\n",
      "In epoch 285, loss: 0.600340723991394\n",
      "In epoch 290, loss: 0.6000540256500244\n",
      "In epoch 295, loss: 0.5998034477233887\n",
      "In epoch 300, loss: 0.5995470285415649\n",
      "In epoch 305, loss: 0.5993064045906067\n",
      "In epoch 310, loss: 0.5990858674049377\n",
      "In epoch 315, loss: 0.5988700985908508\n",
      "In epoch 320, loss: 0.5987225770950317\n",
      "In epoch 325, loss: 0.598468542098999\n",
      "In epoch 330, loss: 0.5982734560966492\n",
      "In epoch 335, loss: 0.598154604434967\n",
      "In epoch 340, loss: 0.5979239344596863\n",
      "In epoch 345, loss: 0.5976948738098145\n",
      "In epoch 350, loss: 0.597515344619751\n",
      "In epoch 355, loss: 0.597322404384613\n",
      "In epoch 360, loss: 0.5971611738204956\n",
      "In epoch 365, loss: 0.5969522595405579\n",
      "In epoch 370, loss: 0.596813976764679\n",
      "In epoch 375, loss: 0.5967115759849548\n",
      "In epoch 380, loss: 0.5964208841323853\n",
      "In epoch 385, loss: 0.5962705612182617\n",
      "In epoch 390, loss: 0.59613436460495\n",
      "In epoch 395, loss: 0.5959307551383972\n",
      "In epoch 400, loss: 0.5958296656608582\n",
      "In epoch 405, loss: 0.595645546913147\n",
      "In epoch 410, loss: 0.595509946346283\n",
      "In epoch 415, loss: 0.5953333377838135\n",
      "In epoch 420, loss: 0.5951874256134033\n",
      "In epoch 425, loss: 0.5950563549995422\n",
      "In epoch 430, loss: 0.5949434638023376\n",
      "In epoch 435, loss: 0.59480881690979\n",
      "In epoch 440, loss: 0.5946813821792603\n",
      "In epoch 445, loss: 0.5946133732795715\n",
      "In epoch 450, loss: 0.5944506525993347\n",
      "In epoch 455, loss: 0.5943422913551331\n",
      "In epoch 460, loss: 0.59424889087677\n",
      "In epoch 465, loss: 0.594123125076294\n",
      "In epoch 470, loss: 0.5940150022506714\n",
      "In epoch 475, loss: 0.5939061641693115\n",
      "In epoch 480, loss: 0.5937954783439636\n",
      "In epoch 485, loss: 0.593675971031189\n",
      "In epoch 490, loss: 0.5935837626457214\n",
      "In epoch 495, loss: 0.593508243560791\n"
     ]
    }
   ],
   "source": [
    "#model = GraphSAGE(g.ndata['feature'].shape[1], 16)\n",
    "\n",
    "model = GCN(2, g.ndata['feature'].shape[1], 16, 16)\n",
    "\n",
    "# You can replace DotPredictor with MLPPredictor.\n",
    "pred = MLPPredictor(16)\n",
    "#pred = DotPredictor()\n",
    "\n",
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "# in this case, loss will in training loop\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.001)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(500):\n",
    "    # forward\n",
    "    h = model(dgl.add_self_loop(g))\n",
    "    \n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "    #print(loss)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {}'.format(e, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC 0.7265529716879391\n",
      "AUC 0.5927757229328681\n",
      "accuracy 0.6089684726048362\n",
      "pass accuracy 0.3021069692058347\n",
      "fail accuracy 0.8834444766599014\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5. check results ------------------------ #\n",
    "from sklearn.metrics import roc_auc_score\n",
    "with torch.no_grad():\n",
    "    pos_score = pred(test_pos_g, h)\n",
    "    neg_score = pred(test_neg_g, h)\n",
    "    \n",
    "    pass_accuracy, fail_accuracy = 0, 0\n",
    "    \n",
    "    predicted_pass = []\n",
    "    for i in pos_score.tolist():\n",
    "        if i > 0.5:\n",
    "            predicted_pass.append(1)\n",
    "            pass_accuracy += 1\n",
    "        else:\n",
    "            predicted_pass.append(0)\n",
    "            \n",
    "    predicted_fail = []\n",
    "    for i in neg_score.tolist():\n",
    "        if i > 0.5:\n",
    "            predicted_fail.append(1)\n",
    "        else:\n",
    "            predicted_fail.append(0)\n",
    "            fail_accuracy += 1\n",
    "    \n",
    "    print('AUC', compute_auc(pos_score, neg_score))\n",
    "    print('AUC', compute_auc(torch.tensor(predicted_pass), torch.tensor(predicted_fail)))\n",
    "    print('accuracy', (pass_accuracy+fail_accuracy)/(len(pos_score)+len(neg_score)))\n",
    "    print('pass accuracy', pass_accuracy/len(pos_score))\n",
    "    print('fail accuracy', fail_accuracy/len(neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
